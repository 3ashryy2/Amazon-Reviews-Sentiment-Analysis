{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.layers import Embedding, SimpleRNN, LSTM, Dense, Dropout, Input, concatenate\n",
    "from keras.utils import to_categorical\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"amazon_reviews.csv\")\n",
    "\n",
    "# Data Pre-processing\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Set stopwords\n",
    "stop_words = set(stopwords.words('english'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for text preprocessing\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):\n",
    "        tokens = word_tokenize(text)\n",
    "        filtered_tokens = [word.lower() for word in tokens if word.isalpha() and word.lower() not in stop_words]\n",
    "        return \" \".join(filtered_tokens)\n",
    "    else:\n",
    "        return \"\"\n",
    "    \n",
    "def save_model(model, model_name):\n",
    "    model_json = model.to_json()\n",
    "    with open(f\"{model_name}.json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(f\"{model_name}.h5\")\n",
    "    print(\"Saved model to disk\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n",
      "Saved model to disk\n",
      "Saved model to disk\n",
      "Saved model to disk\n",
      "Saved model to disk\n",
      "Saved model to disk\n",
      "Saved model to disk\n",
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Loop over different splitting ratios and sequence padding lengths\n",
    "results = []\n",
    "results_rnn = []\n",
    "results_lstm = []\n",
    "for split_ratio in [0.7, 0.8]:\n",
    "    for padding_length in [50, 100]:\n",
    "        # Data Splitting\n",
    "        X_text_train, X_text_test, y_train, y_test = train_test_split(\n",
    "            data['cleaned_review'], data['sentiments'], test_size=(1 - split_ratio), random_state=42)\n",
    "\n",
    "        # Text Preprocessing\n",
    "        X_text_train = X_text_train.apply(preprocess_text)\n",
    "        X_text_test = X_text_test.apply(preprocess_text)\n",
    "\n",
    "        # Tokenization and Padding for text\n",
    "        max_words = 10000\n",
    "        tokenizer = Tokenizer(num_words=max_words)\n",
    "        tokenizer.fit_on_texts(X_text_train)\n",
    "\n",
    "        X_train_seq = tokenizer.texts_to_sequences(X_text_train)\n",
    "        X_test_seq = tokenizer.texts_to_sequences(X_text_test)\n",
    "\n",
    "        X_train_padded = pad_sequences(X_train_seq, maxlen=padding_length)\n",
    "        X_test_padded = pad_sequences(X_test_seq, maxlen=padding_length)\n",
    "\n",
    "        # Convert sentiments to categorical\n",
    "        class_mapping = {'neutral': 0, 'positive': 1, 'negative': 2}\n",
    "        y_train = y_train.map(class_mapping)\n",
    "        y_test = y_test.map(class_mapping)\n",
    "\n",
    "        y_train = to_categorical(y_train)\n",
    "        y_test = to_categorical(y_test)\n",
    "\n",
    "        # Model Training - Simple RNN\n",
    "        # Define Simple RNN model\n",
    "        text_input = Input(shape=(padding_length,), dtype='int32', name='text_input')\n",
    "\n",
    "\n",
    "        embedding_layer = Embedding(max_words, 128, input_length=padding_length)(text_input)\n",
    "        rnn_layer = SimpleRNN(128)(embedding_layer)\n",
    "\n",
    "       \n",
    "        output = Dense(3, activation='softmax')(rnn_layer)\n",
    "\n",
    "        model_rnn = Model(inputs=text_input, outputs=output)\n",
    "        model_rnn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        # Train the Simple RNN model\n",
    "        model_rnn.fit(X_train_padded, y_train, epochs=5, batch_size=64, verbose=0)\n",
    "\n",
    "        # Evaluate the Simple RNN model\n",
    "        loss, accuracy = model_rnn.evaluate(X_test_padded, y_test, verbose=0)\n",
    "\n",
    "        # Save Simple RNN results\n",
    "        results_rnn.append({'split_ratio': split_ratio,\n",
    "                            'padding_length': padding_length,\n",
    "                            'accuracy': accuracy})\n",
    "\n",
    "        # Model Training - LSTM\n",
    "        # Define LSTM model\n",
    "        lstm_input = Input(shape=(padding_length,), dtype='int32', name='lstm_input')\n",
    "\n",
    "        embedding_layer = Embedding(max_words, 128, input_length=padding_length)(lstm_input)\n",
    "        lstm_layer = LSTM(128)(embedding_layer)\n",
    "\n",
    "        output = Dense(3, activation='softmax')(lstm_layer)\n",
    "\n",
    "        model_lstm = Model(inputs=lstm_input, outputs=output)\n",
    "        model_lstm.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        # Train the LSTM model\n",
    "        model_lstm.fit(X_train_padded, y_train, epochs=5, batch_size=64, verbose=0)\n",
    "\n",
    "        # Evaluate the LSTM model\n",
    "        loss, accuracy = model_lstm.evaluate(X_test_padded, y_test, verbose=0)\n",
    "\n",
    "        # Save LSTM results\n",
    "        results_lstm.append({'split_ratio': split_ratio,\n",
    "                             'padding_length': padding_length,\n",
    "                             'accuracy': accuracy})\n",
    "\n",
    "        # Save results\n",
    "        results.append({'split_ratio': split_ratio,\n",
    "                        'padding_length': padding_length,\n",
    "                        'accuracy': accuracy})\n",
    "        \n",
    "        # Save the model\n",
    "\n",
    "        #save_model(model_lstm, f\"model_lstm_{split_ratio}_{padding_length}\")\n",
    "        #save_model(model_rnn, f\"model_rnn_{split_ratio}_{padding_length}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results Summary (LSTM):\n",
      "   split_ratio  padding_length  accuracy\n",
      "0          0.7              50  0.864693\n",
      "1          0.7             100  0.857390\n",
      "2          0.8              50  0.868224\n",
      "3          0.8             100  0.874279\n",
      "\n",
      "Results Summary (RNN):\n",
      "   split_ratio  padding_length  accuracy\n",
      "0          0.7              50  0.853546\n",
      "1          0.7             100  0.848741\n",
      "2          0.8              50  0.860150\n",
      "3          0.8             100  0.854095\n"
     ]
    }
   ],
   "source": [
    "# Convert results to DataFrame\n",
    "results_df_lstm = pd.DataFrame(results)\n",
    "results_df_rnn = pd.DataFrame(results_rnn)\n",
    "\n",
    "# Print results\n",
    "print(\"Results Summary (LSTM):\")\n",
    "print(results_df_lstm)\n",
    "\n",
    "print(\"\\nResults Summary (RNN):\")\n",
    "print(results_df_rnn)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best LSTM model: split_ratio         0.800000\n",
      "padding_length    100.000000\n",
      "accuracy            0.874279\n",
      "Name: 3, dtype: float64\n",
      "Best RNN model: split_ratio        0.80000\n",
      "padding_length    50.00000\n",
      "accuracy           0.86015\n",
      "Name: 2, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Find the best performing model\n",
    "best_model_lstm = results_df_lstm.sort_values(by=['accuracy'], ascending=False).iloc[0]\n",
    "best_model_rnn = results_df_rnn.sort_values(by=['accuracy'], ascending=False).iloc[0]\n",
    "\n",
    "# Print the best performing model\n",
    "print(f\"Best LSTM model: {best_model_lstm}\")\n",
    "print(f\"Best RNN model: {best_model_rnn}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
